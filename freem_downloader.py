import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import re
from urllib.parse import urljoin, urlparse
import time
import math

# å…¨å±€é…ç½®
MAX_WORKERS = 12              # å¹¶å‘çº¿ç¨‹æ•°ï¼ˆå»ºè®®æ ¹æ®å¸¦å®½è°ƒæ•´ï¼‰
CHUNK_SIZE = 1024 * 128       # 128KB å—å¤§å°ï¼ˆå‡å°‘IOæ¬¡æ•°ï¼‰
TIMEOUT = 30                  # å•æ–‡ä»¶è¶…æ—¶ï¼ˆç§’ï¼‰
RETRIES = 3                   # å¤±è´¥é‡è¯•æ¬¡æ•°
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"

def get_http_session():
    """åˆ›å»ºé«˜æ€§èƒ½HTTPä¼šè¯"""
    session = requests.Session()
    session.headers.update({
        "User-Agent": USER_AGENT,
        "Accept-Encoding": "gzip, deflate",  # å¯ç”¨å‹ç¼©
    })
    # æ‰©å¤§è¿æ¥æ± 
    adapter = requests.adapters.HTTPAdapter(
        pool_connections=MAX_WORKERS,
        pool_maxsize=MAX_WORKERS * 2
    )
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

def format_speed(speed):
    """æ ¼å¼åŒ–é€Ÿåº¦æ˜¾ç¤º"""
    if speed >= 1024 * 1024:
        return f"{speed/(1024 * 1024):.2f} MB/s"
    return f"{speed/1024:.2f} KB/s"

def download_file(session, url, local_path):
    """å¢å¼ºç‰ˆä¸‹è½½å‡½æ•°ï¼ˆå¸¦é€Ÿåº¦æ˜¾ç¤ºï¼‰"""
    if os.path.exists(local_path):
        print(f"[è·³è¿‡] æ–‡ä»¶å·²å­˜åœ¨: {os.path.basename(local_path)}")
        return True

    for attempt in range(RETRIES):
        try:
            start_time = time.time()
            downloaded = 0
            last_update = start_time

            with session.get(url, stream=True, timeout=TIMEOUT) as r:
                r.raise_for_status()
                total_size = int(r.headers.get('content-length', 0))

                with open(local_path, 'wb') as f:
                    for chunk in r.iter_content(chunk_size=CHUNK_SIZE):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            
                            # æ¯ç§’æ›´æ–°æ˜¾ç¤º
                            now = time.time()
                            if now - last_update >= 0.5:
                                elapsed = now - start_time
                                speed = downloaded / elapsed if elapsed > 0 else 0
                                progress = f"{downloaded/1024/1024:.2f}MB" if total_size ==0 else \
                                         f"{downloaded/1024/1024:.2f}/{total_size/1024/1024:.2f}MB"
                                print(
                                    f"\r[ä¸‹è½½] {os.path.basename(local_path)} "
                                    f"{progress} | {format_speed(speed)}".ljust(80),
                                    end=""
                                )
                                last_update = now

            # æœ€ç»ˆé€Ÿåº¦è®¡ç®—
            elapsed = time.time() - start_time
            if elapsed == 0:
                elapsed = 0.001
            speed = downloaded / elapsed
            print(f"\r[å®Œæˆ] {os.path.basename(local_path)} "
                f"({downloaded/1024/1024:.2f}MB @ {format_speed(speed)})".ljust(80))
            return True

        except Exception as e:
            print(f"\r[é‡è¯• {attempt+1}/{RETRIES}] {url} é”™è¯¯: {str(e)}")
            time.sleep(1)
    
    return False

def crawl_from_number(base_url, start_num, save_dir):
    """é«˜æ€§èƒ½çˆ¬å–"""
    session = get_http_session()
    current_num = start_num

    while True:
        dir_url = urljoin(base_url, f"{current_num}/")
        print(f"\nğŸ” æ£€æŸ¥ç›®å½•: {dir_url}")

        try:
            # å¿«é€Ÿç›®å½•æ£€æµ‹
            r = session.head(dir_url, timeout=5)
            if r.status_code != 200:
                print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {current_num}")
                current_num += 1
                continue

            # è·å–æ–‡ä»¶åˆ—è¡¨
            r = session.get(dir_url, timeout=TIMEOUT)
            soup = BeautifulSoup(r.text, 'html.parser')
            files = []
            for a in soup.find_all('a'):
                href = a.get('href')
                if href and not href.startswith(('../', './')) and not href.endswith('/'):
                    files.append(urljoin(dir_url, href))

            if not files:
                print(f"âš ï¸ ç©ºç›®å½•: {current_num}")
                current_num += 1
                continue

            # å¹¶å‘ä¸‹è½½
            os.makedirs(os.path.join(save_dir, str(current_num)), exist_ok=True)
            print(f"ğŸ“ å¼€å§‹ä¸‹è½½ç›®å½• {current_num} ({len(files)} ä¸ªæ–‡ä»¶)")
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = []
                for file_url in files:
                    local_path = os.path.join(save_dir, str(current_num), 
                                            os.path.basename(urlparse(file_url).path))
                    futures.append(executor.submit(download_file, session, file_url, local_path))

                # å®æ—¶æ˜¾ç¤ºè¿›åº¦
                for future in as_completed(futures):
                    future.result()

            print(f"âœ… å®Œæˆç›®å½•: {current_num}")
            current_num += 1

        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {str(e)}")
            current_num += 1

if __name__ == "__main__":
    print("ğŸš€ FreeM é«˜æ€§èƒ½ä¸‹è½½å™¨ v2")
    BASE_URL = "http://ftp.airnet.ne.jp/pub/pc/freem/"
    SAVE_DIR = input("ä¿å­˜è·¯å¾„ï¼ˆä¾‹å¦‚ D:/freemï¼‰: ").strip()
    START_NUM = int(input("èµ·å§‹ç¼–å·ï¼ˆä¾‹å¦‚ 731ï¼‰: ") or 1)

    start_time = time.time()
    crawl_from_number(BASE_URL, START_NUM, SAVE_DIR)
    print(f"â±ï¸ æ€»è€—æ—¶: {time.time()-start_time:.2f}ç§’")